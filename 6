from sklearn.datasets import load_iris

from sklearn.impute import SimpleImputer

import numpy as np

import pandas as pd

from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import MinMaxScaler

iris = load_iris()

x = pd.DataFrame(iris.data, columns=iris.feature_names)

 #a)Handling Missing values

missing_data = x.copy()

missing_data.loc[2:3, 'sepal width (cm)'] = np.nan 

print("Data before imputation:")

print(missing_data.head()) 

imputer = SimpleImputer(strategy='median') 

data_imputed = imputer.fit_transform(missing_data) 

imputed_data_df = pd.DataFrame(data_imputed, columns=iris.feature_names)

print("Data after imputation")

print(imputed_data_df.head())

 #b)Label encoding

iris_df = pd.read_csv("Desktop/iris.csv") 

 y = iris_df["species"]

 print("Unique categories before label encoding:")

print(set(y))
label_encoder = LabelEncoder()
target_encoded = label_encoder.fit_transform(y)
print("\nUnique categories after label encoding:")
print(set(target_encoded))
 #c)Perform feature scaling
scaler=MinMaxScaler() data_normalized=scaler.fit_transform(x)
normalized_data_df=pd.DataFrame(data_normalized,columns=iris.feature_names)
print("\nData after feature scaling-Normalization:")
print(normalized_data_df.head())
